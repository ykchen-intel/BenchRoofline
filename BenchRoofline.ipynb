{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9664ab8",
   "metadata": {},
   "source": [
    "## Overall plan\n",
    "# Setup\n",
    "    Variables: the input/output token length, bytes_per_value, types of layers (Done)\n",
    "    Variables: parallelism (P4)\n",
    "    Mixed-precision (P6)\n",
    "# Read config.json\n",
    "    Read from local file (Done)\n",
    "    Read from HuggingFace (P5)\n",
    "# Read hardware specification\n",
    "    Read from Excel spreadsheet (P2)\n",
    "# Breakdown the models into different kernels/tensors\n",
    "    Manually breakdown the GQA models (done)\n",
    "    MOE models (P3)\n",
    "    Automatic model conversion (P4)\n",
    "# Calculate time per kernels \n",
    "    Simple roofline (done)\n",
    "    Theoretical tiling (done)\n",
    "    Hierarchical cache (P3)\n",
    "    Realistic tiling (P4)\n",
    "# Output \n",
    "    Breakdown between different kernels and the shape of the tensors: Mw_Ma, Ma_Ma, Mw_Va, Vw_Va, Va_Va (Done)\n",
    "    Estimated execution time for different kernels (Done)\n",
    "    Estimated activities for different kernels for power analysis (P6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37cc10",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59972076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Setup\n",
    "# Variables: the input/output token length\n",
    "token_lengths = [\n",
    "    (1024, 1024),\n",
    "    (8192, 1024)\n",
    "]\n",
    "# Variables: workload characteristics\n",
    "bytes_per_value = 2\n",
    "# To-do list: mixed precision between weights and activations (P6)\n",
    "\n",
    "# Variables: parallelism (P4)\n",
    "\n",
    "# Variables: types of layers\n",
    "Ma_Mw = \"Ma_Mw\" # The first matrix is the activations, the second matrix is the weights\n",
    "Ma_Mqa = \"Ma_Mqa\" # The first matrix is the activations, the second matrix is the grouped activations (special usages for QGA)\n",
    "Mw_Ma = \"Mw_Ma\" # The first matrix is the weights, the second matrix is the activations\n",
    "Ma_Ma = \"Ma_Ma\" # The first matrix is the activations, the second matrix is the activations\n",
    "\n",
    "Va_Mw = \"Va_Mw\" # The first vector is the activations, the second matrix is the weights\n",
    "Va_Mqa = \"Va_Mqa\" # The first vector is the activations, the second matrix is the grouped activations (special usages for QGA)\n",
    "Vw_Va = \"Vw_Va\" # The first vector is the weights, the second vector is the activations\n",
    "Va_Va = \"Va_Va\" # The first vector is the activations, the second vector is the activations\n",
    "\n",
    "KVCache = \"KVCache\" # This is to capture the retrieval of the key/value cache\n",
    "Async_KVCache = \"Async_KVCache\" # This is to capture the store (and some retrieval) of the key/value cache\n",
    "\n",
    "# Flags for debug and detailed print outputs\n",
    "DEBUG_PRINT = True\n",
    "DETAIL_PRINT = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57903e3f",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3dfcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts an integer number into a size string with M, G, T, P suffix.\n",
    "def convert_number_to_string(count):\n",
    "    # Define the suffixes for bytes, terabytes, and petabytes\n",
    "    suffixes = [\"M\", \"G\", \"T\", \"P\"]\n",
    "    # Define the corresponding byte multiples\n",
    "    multiples = [1024**2, 1024**3, 1024**4, 1024**5]\n",
    "\n",
    "    # Iterate over the multiples in reverse to find the largest fitting multiple\n",
    "    for i in reversed(range(len(multiples))):\n",
    "        if count >= multiples[i]:\n",
    "            size = count / multiples[i]  # Calculate the size in the corresponding unit\n",
    "            return f\"{size:.1f}{suffixes[i]}\"  # Return the formatted size string\n",
    "    # If the count is less than 1B, return it as a string\n",
    "    return str(count)\n",
    "\n",
    "# Function to print a list with a given name\n",
    "def print_list(name, list):\n",
    "    print(\"\\t==\", name, \"==\")\n",
    "    for item in list:\n",
    "        print(\"\\t\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102ad19",
   "metadata": {},
   "source": [
    "Read model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16746ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model config from HuggingFace.\n",
    "# If you get a permission error see: https://huggingface.co/docs/transformers.js/en/guides/private\n",
    "#!pip install --upgrade huggingface_hub\n",
    "\n",
    "model_repo_id = \"meta-llama/Llama-3.1-8B\" # https://huggingface.co/meta-llama/Llama-3.1-8B\n",
    "\n",
    "import json\n",
    "from huggingface_hub import HfApi, hf_hub_download, get_collection\n",
    "api = HfApi()\n",
    "\n",
    "downloaded_path = hf_hub_download(repo_id=model_repo_id, filename=\"config.json\")\n",
    "with open(downloaded_path, \"r\") as file:\n",
    "    data = json.load(file)  # Parse the JSON file into a Python dictionary\n",
    "\n",
    "# Extract relevant values from the JSON data\n",
    "class LLMConfig:\n",
    "    def __init__(self, data):\n",
    "        self.hidden_size = data[\"hidden_size\"]\n",
    "        self.intermediate_size = data[\"intermediate_size\"]\n",
    "        self.num_q_heads = data[\"num_attention_heads\"]\n",
    "        self.num_kv_heads = data[\"num_key_value_heads\"]\n",
    "        self.num_hidden_layers = data[\"num_hidden_layers\"]\n",
    "        self.sub_dmodel = self.hidden_size // self.num_q_heads\n",
    "        self.grouped_q = self.num_q_heads / self.num_kv_heads\n",
    "\n",
    "llm = LLMConfig(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37659421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a912bc",
   "metadata": {},
   "source": [
    "Read hardware specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "968a4da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected architecture: NVL-AX\n",
      "Num_Cores            3.200000e+01\n",
      "Frequency            2.500000e+09\n",
      "MACs_per_cycle       2.048000e+03\n",
      "Bandwidth            3.672197e+11\n",
      "L3_cache_capacity    3.774874e+07\n",
      "MACs_per_second      1.638400e+14\n",
      "Name: NVL-AX, dtype: float64\n",
      "MACs_per_second: 163840000000000.0\n"
     ]
    }
   ],
   "source": [
    "# To-Do: Read detailed hardware specification from spreadsheet (P2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define hardware specs for multiple architectures\n",
    "HW_SPEC_DF = pd.DataFrame({\n",
    "    \"NVL-AX\": {\n",
    "        \"Num_Cores\": 32,                        # Number of cores\n",
    "        \"Frequency\": 2.5 * 1e9,                 # Frequency in Hz\n",
    "        \"MACs_per_cycle\": 2*1024,               # 2K MACs per cycle per core for FP16 (depth = 8)\n",
    "        \"Bandwidth\": 342*1024*1024*1024,        # 342 GB/s \n",
    "        \"L3_cache_capacity\": 36*1024*1024,      # 36 MB\n",
    "    },\n",
    "    \"JGS\": {\n",
    "        \"Num_Cores\": 192,                       # Number of cores\n",
    "        \"Frequency\": 1.5 * 1e9,                 # Frequency in Hz\n",
    "        \"MACs_per_cycle\": 8*1024,               # 8K MACs per cycle per core for FP32\n",
    "        \"Bandwidth\": 30*1024*1024*1024*1024,    # 30 TB/s \n",
    "        \"L3_cache_capacity\": 120*1024*1024,     # 120 MB\n",
    "    }\n",
    "})\n",
    "\n",
    "# Add derived row for MACs_per_second\n",
    "HW_SPEC_DF.loc[\"MACs_per_second\"] = (\n",
    "    HW_SPEC_DF.loc[\"Num_Cores\"] * HW_SPEC_DF.loc[\"Frequency\"] * HW_SPEC_DF.loc[\"MACs_per_cycle\"]\n",
    ")\n",
    "\n",
    "# Select active architecture\n",
    "arch = \"NVL-AX\"  # or \"JGS\"\n",
    "HW_SPEC = HW_SPEC_DF[arch]\n",
    "\n",
    "# Example usage:\n",
    "if DEBUG_PRINT:\n",
    "    print(\"Selected architecture:\", arch)\n",
    "    print(HW_SPEC)\n",
    "    print(\"MACs_per_second:\", HW_SPEC[\"MACs_per_second\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a1c7d",
   "metadata": {},
   "source": [
    "Breakdown the models into different kernels/tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d792cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /mnt/data2/parviz/notebooks/.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/data2/parviz/notebooks/.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/data2/parviz/notebooks/.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /mnt/data2/parviz/notebooks/.venv/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/data2/parviz/notebooks/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/data2/parviz/notebooks/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dad35895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do: (1) MOE model, (2) auto-conversion, (3) parallelism \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class WorkloadProfile:\n",
    "    def __init__(self):\n",
    "        # Set max_colwidth to None to display full content without truncation\n",
    "        pd.set_option('display.max_colwidth', 40)\n",
    "        self.layers = pd.DataFrame(columns=[\n",
    "            \"type\", \"name\", \"m\", \"k\", \"n\", \"batch_size\", \"grouped_q\", \"compute\", \"weight\", \"kv_cache\"\n",
    "        ])\n",
    "        self.total_compute = 0\n",
    "        self.total_weight = 0\n",
    "        self.total_kv_cache = 0\n",
    "\n",
    "    def add_layer(self, layer_type, name, *args, **kwargs):\n",
    "        dim_m = args[0]\n",
    "        dim_k = args[1]\n",
    "        dim_n = args[2]\n",
    "        batch_size = args[3]\n",
    "        grouped_q = kwargs.get('grouped_q', 1)\n",
    "        compute = None\n",
    "        weight = None\n",
    "        kv_cache = None\n",
    "        if grouped_q > 1 and layer_type not in [\"Ma_Mqa\", \"Va_Mqa\"]:\n",
    "            raise ValueError(f\"Invalid layer type {layer_type} for grouped_q > 1\")\n",
    "        match layer_type:\n",
    "            case \"Ma_Mw\":\n",
    "                compute = dim_m * dim_k * dim_n * batch_size\n",
    "                weight = dim_k * dim_n\n",
    "            case \"Ma_Mqa\":\n",
    "                compute = dim_m * grouped_q * dim_k * dim_n * batch_size\n",
    "            case \"Mw_Ma\":\n",
    "                compute = dim_m * dim_k * dim_n * batch_size\n",
    "                weight = dim_m * dim_k\n",
    "            case \"Ma_Ma\":\n",
    "                compute = dim_m * dim_k * dim_n * batch_size\n",
    "            case \"Va_Mw\":\n",
    "                compute = dim_m * dim_k * dim_n * batch_size\n",
    "                weight = dim_k * dim_n\n",
    "            case \"Va_Mqa\":\n",
    "                compute = dim_m * grouped_q * dim_k * dim_n * batch_size\n",
    "            case \"Vw_Va\":\n",
    "                compute = dim_m * dim_k * dim_n * batch_size\n",
    "                weight = dim_m * dim_k\n",
    "            case \"Va_Va\":\n",
    "                compute = dim_m * dim_k * dim_n * batch_size\n",
    "            case \"KVCache\":\n",
    "                kv_cache = dim_m * dim_k * dim_n * batch_size\n",
    "            case \"Async_KVCache\":\n",
    "                kv_cache = dim_m * dim_k * dim_n * batch_size\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown layer_type: {layer_type}\")\n",
    "        new_row = {\n",
    "            \"type\": layer_type,\n",
    "            \"name\": name,\n",
    "            \"m\": dim_m,\n",
    "            \"k\": dim_k,\n",
    "            \"n\": dim_n,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"grouped_q\": grouped_q,\n",
    "            \"compute\": compute,\n",
    "            \"weight\": weight,\n",
    "            \"kv_cache\": kv_cache\n",
    "        }\n",
    "        self.layers = pd.concat([self.layers, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    def sum_totals(self):\n",
    "        self.layers[\"compute\"] = self.layers[\"compute\"].infer_objects(copy=False)\n",
    "        self.total_compute = self.layers[\"compute\"].fillna(0).sum()\n",
    "        self.layers[\"weight\"] = self.layers[\"weight\"].infer_objects(copy=False)\n",
    "        self.total_weight = self.layers[\"weight\"].fillna(0).sum()\n",
    "        self.layers[\"kv_cache\"] = self.layers[\"kv_cache\"].infer_objects(copy=False)\n",
    "        self.total_kv_cache = self.layers[\"kv_cache\"].fillna(0).sum()\n",
    "\n",
    "def perform_prefill(llm, seq_length, p_batch_size):\n",
    "    # Note some parts of the transformer are not modeled, e.g., \n",
    "    # Input and position embedding\n",
    "    # Softmax\n",
    "    # Residual connection and layer normalization\n",
    "    # Task-specific output layer\n",
    "\n",
    "    # Create a WorkloadProfile instance\n",
    "    wl = WorkloadProfile()\n",
    "\n",
    "    # Add layers using the WorkloadProfile object\n",
    "    wl.add_layer(Ma_Mw, \"Q/K/V*W\", seq_length, llm.hidden_size, llm.sub_dmodel * (llm.num_q_heads+llm.num_kv_heads*2), p_batch_size)\n",
    "    wl.add_layer(Ma_Mqa, \"Q*gK\", seq_length, llm.hidden_size, seq_length, p_batch_size, qrouped_q=llm.grouped_q)\n",
    "    wl.add_layer(Ma_Mqa, \"Q*gK*gV\", seq_length, llm.hidden_size, seq_length, p_batch_size, qrouped_q=llm.grouped_q)\n",
    "    wl.add_layer(Ma_Mw, \"O*W\", seq_length, llm.hidden_size, llm.hidden_size, p_batch_size)\n",
    "    wl.add_layer(Ma_Mw, \"FFN_up\", seq_length, llm.hidden_size, llm.intermediate_size, p_batch_size)\n",
    "    wl.add_layer(Ma_Mw, \"FFN_gate\", seq_length, llm.hidden_size, llm.intermediate_size, p_batch_size)\n",
    "    wl.add_layer(Ma_Mw, \"FFN_down\", seq_length, llm.intermediate_size, llm.hidden_size, p_batch_size)\n",
    "    # add memory to KV cache #### More optimization opportunity (P1)\n",
    "    wl.add_layer(Async_KVCache, \"KVCache_Store\", seq_length, llm.sub_dmodel, 2*llm.num_kv_heads, p_batch_size)\n",
    "\n",
    "    # Sum up the compute and memory usage\n",
    "    wl.sum_totals()\n",
    "\n",
    "    if DEBUG_PRINT:\n",
    "        print(\"Prefill Compute\", wl.total_compute)\n",
    "        print(\"Prefill Weights\", wl.total_weight)\n",
    "        print(\"Prefill KV cache\", wl.total_kv_cache)\n",
    "        print(wl.layers)\n",
    "\n",
    "\n",
    "    # Print detailed information if the flag is set\n",
    "    if DETAIL_PRINT:\n",
    "        print(\"\")\n",
    "        print(\"   Details:\")\n",
    "        print(\"   === Input parameters ===\")\n",
    "        print(\"   Context length:\", seq_length)\n",
    "        print(\"   Bytes per FP value:\", bytes_per_value) # To-Do: this is set globally, not specific to the model or workload (P2)\n",
    "        print(\"   Layers:\", llm.num_hidden_layers)\n",
    "\n",
    "        print(\"   Total Compute (MACs):\", convert_number_to_string(wl.total_compute * llm.num_hidden_layers))\n",
    "        print(\"   Total Weights Footprint (Bytes):\", convert_number_to_string(wl.total_weight * llm.num_hidden_layers))\n",
    "        print(\"   Total KV Cache Footprint (Bytes):\", convert_number_to_string(wl.total_kv_cache * llm.num_hidden_layers))\n",
    "\n",
    "    return wl\n",
    "\n",
    "def perform_decode(llm, context_length, d_batch_size): # Decoding one token at a time\n",
    "    # Create a WorkloadProfile instance\n",
    "    wl = WorkloadProfile()\n",
    "\n",
    "    # Add layers using the WorkloadProfile object\n",
    "    wl.add_layer(Va_Mw, \"Q/K/V*W\", 1, llm.hidden_size, llm.sub_dmodel * (llm.num_q_heads+2*llm.num_kv_heads), d_batch_size)\n",
    "    wl.add_layer(KVCache, \"KVCache_Read\", context_length, llm.sub_dmodel, 2*llm.num_kv_heads, d_batch_size)\n",
    "    wl.add_layer(Va_Mqa, \"Q*gK\", 1, llm.hidden_size, context_length, d_batch_size, qrouped_q=llm.grouped_q)\n",
    "    wl.add_layer(Va_Mqa, \"Q*gK*gV\", 1, llm.hidden_size, context_length, d_batch_size, qrouped_q=llm.grouped_q)\n",
    "    wl.add_layer(Va_Mw, \"O*W\", 1, llm.hidden_size, llm.hidden_size, d_batch_size)\n",
    "    wl.add_layer(Va_Mw, \"FFN_up\", 1, llm.hidden_size, llm.intermediate_size, d_batch_size)\n",
    "    wl.add_layer(Va_Mw, \"FFN_gate\", 1, llm.hidden_size, llm.intermediate_size, d_batch_size)\n",
    "    wl.add_layer(Va_Mw, \"FFN_down\", 1, llm.intermediate_size, llm.hidden_size, d_batch_size)\n",
    "    # add the last token to KV cache #### More optimization opportunity (P1)\n",
    "\n",
    "    # Sum up the compute and memory usage\n",
    "    wl.sum_totals()\n",
    "\n",
    "    if DEBUG_PRINT:\n",
    "        print(\"Decode Compute\", wl.total_compute)\n",
    "        print(\"Decode Weights\", wl.total_weight)\n",
    "        print(\"Decode KV cache\", wl.total_kv_cache)\n",
    "        print(wl.layers)\n",
    "\n",
    "\n",
    "    # Print detailed information if the flag is set\n",
    "    if DETAIL_PRINT:\n",
    "        print(\"\")\n",
    "        print(\"   Details:\")\n",
    "        print(\"   === Input parameters ===\")\n",
    "        print(\"   Context length:\", context_length)\n",
    "        print(\"   Bytes per FP value:\", bytes_per_value) # To-Do: this is set globally, not specific to the model or workload (P2)\n",
    "        print(\"   Layers:\", llm.num_hidden_layers)\n",
    "\n",
    "        print(\"   Total Compute (MACs):\", convert_number_to_string(wl.total_compute * llm.num_hidden_layers))\n",
    "        print(\"   Total Weights Footprint (Bytes):\", convert_number_to_string(wl.total_weight * llm.num_hidden_layers))\n",
    "        print(\"   Total KV Cache Footprint (Bytes):\", convert_number_to_string(wl.total_kv_cache * llm.num_hidden_layers))\n",
    "\n",
    "    return wl\n",
    "\n",
    "def perform_chunkedprefill(llm, p_context_length, p_tokens, d_context_length, d_tokens): # Decoding one token at a time\n",
    "    # Create a WorkloadProfile instance\n",
    "    wl = WorkloadProfile()\n",
    "\n",
    "    # Add layers using the profile object\n",
    "    wl.add_layer(Ma_Mw, \"Q/K/V*W\", (p_tokens+d_tokens), llm.hidden_size, llm.sub_dmodel * (llm.num_q_heads+llm.num_kv_heads*2), 1)\n",
    "\n",
    "    # decode token attention\n",
    "    wl.add_layer(KVCache, \"dKVCache_Read\", d_context_length, llm.sub_dmodel, 2*llm.num_kv_heads, d_tokens)\n",
    "    wl.add_layer(Va_Mqa, \"dQ*gK\", 1, llm.hidden_size, llm.sub_dmodel * llm.num_kv_heads, d_tokens, qrouped_q=llm.grouped_q)\n",
    "    wl.add_layer(Va_Mqa, \"dQ*gK*gV\", 1, llm.hidden_size, llm.sub_dmodel * llm.num_kv_heads, d_tokens, qrouped_q=llm.grouped_q)\n",
    "\n",
    "    # prefill token attention\n",
    "    wl.add_layer(Async_KVCache, \"pKVCache_Read\", p_context_length, llm.sub_dmodel, 2*llm.num_kv_heads, 1)\n",
    "    wl.add_layer(Ma_Mqa, \"pQ*gK\", p_tokens, llm.hidden_size, p_context_length, 1, qrouped_q=llm.grouped_q)\n",
    "    wl.add_layer(Ma_Mqa, \"pQ*gK*gV\", p_tokens, llm.hidden_size, p_context_length, 1, qrouped_q=llm.grouped_q)\n",
    "    wl.add_layer(Async_KVCache, \"KVCache_Store\", p_tokens, llm.sub_dmodel, 2*llm.num_kv_heads, 1)\n",
    "\n",
    "    wl.add_layer(Ma_Mw, \"O*W\", (p_tokens+d_tokens), llm.hidden_size, llm.hidden_size, 1)\n",
    "    wl.add_layer(Ma_Mw, \"FFN_up\", (p_tokens+d_tokens), llm.hidden_size, llm.intermediate_size, 1)\n",
    "    wl.add_layer(Ma_Mw, \"FFN_gate\", (p_tokens+d_tokens), llm.hidden_size, llm.intermediate_size, 1)\n",
    "    wl.add_layer(Ma_Mw, \"FFN_down\", (p_tokens+d_tokens), llm.intermediate_size, llm.hidden_size, 1)\n",
    "\n",
    "    # Sum up the compute and memory usage\n",
    "    wl.sum_totals()\n",
    "\n",
    "    if DEBUG_PRINT:\n",
    "        print(\"Chunked Compute\", wl.total_compute)\n",
    "        print(\"Chunked Weights\", wl.total_weight)\n",
    "        print(\"Chunked KV cache\", wl.total_kv_cache)\n",
    "        print(wl.layers)\n",
    "\n",
    "    # Print detailed information if the flag is set\n",
    "    if DETAIL_PRINT:\n",
    "        print(\"\")\n",
    "        print(\"   Details:\")\n",
    "        print(\"   === Input parameters ===\")\n",
    "        print(\"   Prefill context length:\", p_context_length)\n",
    "        print(\"   Decode context length:\", d_context_length)\n",
    "        print(\"   Bytes per FP value:\", bytes_per_value) # To-Do: this is set globally, not specific to the model or workload (P2)\n",
    "        print(\"   Layers:\", llm.num_hidden_layers)\n",
    "\n",
    "        print(\"   Total Compute (MACs):\", convert_number_to_string(wl.total_compute * llm.num_hidden_layers))\n",
    "        print(\"   Total Weights Footprint (Bytes):\", convert_number_to_string(wl.total_weight * llm.num_hidden_layers))\n",
    "        print(\"   Total KV Cache Footprint (Bytes):\", convert_number_to_string(wl.total_kv_cache * llm.num_hidden_layers))\n",
    "\n",
    "    return wl\n",
    "\n",
    "def perform_GEMM(dim_m, dim_k, dim_n): # Decoding one token at a time\n",
    "    # Create a WorkloadProfile instance\n",
    "    wl = WorkloadProfile()\n",
    "\n",
    "    # Add layers using the profile object\n",
    "    wl.add_layer(Ma_Ma, \"GEMM\", dim_m, dim_k, dim_n, 1)\n",
    "\n",
    "    return wl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050367e0",
   "metadata": {},
   "source": [
    "Kernel breakdowns\n",
    "\n",
    "# DataFrame-based kernel time estimation and output\n",
    "\n",
    "def estimate_kernel_times_df(wl, HW_SPEC):\n",
    "    df = wl.layers.copy()\n",
    "    df['compute_time_s'] = df['compute'].fillna(0) / HW_SPEC['MACs_per_second']\n",
    "    df['memory_time_s'] = (df['weight'].fillna(0) + df['kv_cache'].fillna(0)) * bytes_per_value / HW_SPEC['Bandwidth']\n",
    "    df['roofline_time_s'] = df[['compute_time_s', 'memory_time_s']].max(axis=1)\n",
    "    return df\n",
    "\n",
    "# Example usage for DataFrame-based output:\n",
    "wl = perform_prefill(llm, 1024, 1)\n",
    "kernel_times_df = estimate_kernel_times_df(wl, HW_SPEC)\n",
    "\n",
    "print(\"Layer-wise kernel time estimates (DataFrame-based):\")\n",
    "print(kernel_times_df[['type', 'name', 'compute', 'weight', 'kv_cache', 'compute_time_s', 'memory_time_s', 'roofline_time_s']])\n",
    "\n",
    "total_time = kernel_times_df['roofline_time_s'].sum()\n",
    "print(f\"Total estimated time: {total_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71b3428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first tuple in token_lengths\n",
    "\n",
    "if 0:\n",
    "#for token_length in token_lengths:\n",
    "    # Unpack the tuple into two variables\n",
    "    input_length, output_length = token_length\n",
    "    print(\"Input length:\", input_length)\n",
    "    print(\"Output length:\", output_length)\n",
    "    p_batch_size = 1\n",
    "    d_batch_size = 256\n",
    "    chunksize = 256\n",
    "\n",
    "    # Perform prefill\n",
    "    perform_prefill(llm, input_length, p_batch_size)\n",
    "    # Perform decode\n",
    "    perform_decode(llm, input_length, d_batch_size)    \n",
    "    # Perform chunked prefill\n",
    "    d_tokens = chunksize * output_length // (input_length + output_length)\n",
    "    p_tokens = chunksize - d_tokens\n",
    "    d_context_length = input_length + output_length // 2    # This is used to approximately the average decode computation\n",
    "    p_context_length = int(input_length / 1.73 - p_tokens//2)             # 1.73 = sqrt(3) This is used to approximately the average prefill computation\n",
    "\n",
    "    perform_chunkedprefill(llm, p_context_length, p_tokens, d_context_length, d_tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd388892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#### Calculate time per kernels \n",
    "# Realistic tiling (P3)\n",
    "\n",
    "def estimate_tiling_sizes(cache_size, bytes_per_value, num_fixed_dims, *dims):\n",
    "    \"\"\"\n",
    "    Estimate tiling sizes for matrix multiplication given cache_size, bytes_per_value, and some fixed dimensions.\n",
    "    If num_fixed_dims > 0, dims should provide the fixed dimensions in order (dim_1, dim_2, dim_3).\n",
    "    The remaining dimensions will be estimated to fit the cache.\n",
    "\n",
    "    Args:\n",
    "        cache_size: total cache size available (e.g., L3 cache)\n",
    "        bytes_per_value: bytes per matrix element\n",
    "        num_fixed_dims: number of fixed dimensions provided (0-2)\n",
    "        *dims: the fixed dimension values (dim_m, dim_k, dim_n) in order\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dim_m, dim_k, dim_n) estimated tile sizes\n",
    "    \"\"\"\n",
    "    match num_fixed_dims:\n",
    "        case 0:\n",
    "            # Assume cache is split equally among the three matrices\n",
    "            size_per_matrix = cache_size / 3\n",
    "            # For cubic root, use **(1/3)\n",
    "            perfect_dim = (size_per_matrix / bytes_per_value) ** (1/2)\n",
    "            return perfect_dim\n",
    "        case 1:\n",
    "            # Solve for x in: x * x + 2 * fixed_dim * x < cache_size / bytes_per_value\n",
    "            # Rearranged: x^2 + 2 * fixed_dim * x - (cache_size / bytes_per_value) < 0\n",
    "            # Use quadratic formula: x = (-b + sqrt(b^2 - 4ac)) / 2a, where a=1, b=2*fixed_dim, c=-(cache_size / bytes_per_value)\n",
    "            b = dims[0]*2\n",
    "            discriminant = b**2 + 4 * (cache_size / bytes_per_value)\n",
    "            dim_2 = (-b + math.sqrt(discriminant)) / 2\n",
    "            return dim_2\n",
    "        case 2:\n",
    "            # Because the third dimension is not fixed, we can use the remaining cache space\n",
    "            # to estimate the third dimension\n",
    "            remaining_capacity = (cache_size - (dims[0] * dims[1] * bytes_per_value))\n",
    "            dim_3 = remaining_capacity / (bytes_per_value * (dims[0]+dims[1]))\n",
    "            return dim_3\n",
    "        \n",
    "# Simple roofline: If matrix-matrix operations, assume compute bound.  If matrix-vector operations, assume memory bound.\n",
    "# Assume wl is a WorkloadProfile instance (from perform_prefill, perform_decode, or perform_chunkedprefill)\n",
    "# HW_SPEC should define MACs_per_second, L3_cache_capacity, Bandwidth, and Frequency\n",
    "import numpy as np\n",
    "def estimate_kernel_times_by_theoretical_tiler(wl, HW_SPEC):\n",
    "    # Theoretical tiling: the best tiling used all the cache space for matrix A, matrix B, and matrix C\n",
    "    # Add new columns if not present\n",
    "    for col, dtype in [\n",
    "        (\"best_tiler\", object),\n",
    "        (\"exec_cycle\", float),\n",
    "        (\"cmp_cycle\", float),\n",
    "        (\"mem_cycle\", float),\n",
    "        (\"mem_cycle_to_cmp_cycle\", float),\n",
    "        (\"bound\", object)\n",
    "    ]:\n",
    "        if col not in wl.layers.columns:\n",
    "            wl.layers[col] = pd.Series([None]*len(wl.layers), dtype=dtype)\n",
    "    for idx, layer in wl.layers.iterrows():\n",
    "        layer_type = layer[\"type\"]\n",
    "        layer_name = layer[\"name\"]\n",
    "        best_tiler = None\n",
    "        cmp_cycle = None\n",
    "        mem_cycle = None\n",
    "        mem_cycle_to_cmp_cycle = None\n",
    "        exec_cycle = None\n",
    "        layer_bound = None\n",
    "        if \"M\" in layer_type or \"w\" in layer_type:\n",
    "            if not pd.isna(layer[\"compute\"]):\n",
    "                L3_cache_capacity = HW_SPEC[\"L3_cache_capacity\"]\n",
    "                wldims = [layer[\"m\"], layer[\"k\"], layer[\"n\"]]\n",
    "                batch_size = layer[\"batch_size\"]\n",
    "\n",
    "                # Step 0: use batch size to adjust dimensions so that the output matrix is as closer to square as possible\n",
    "                if batch_size > 1:\n",
    "                    if wldims[0] > wldims[2] and wldims[0] / wldims[2] > batch_size:\n",
    "                        wldims[2] = wldims[2] * batch_size\n",
    "                    elif wldims[2] > wldims[0] and wldims[2] / wldims[0] > batch_size:\n",
    "                        wldims[0] = wldims[0] * batch_size\n",
    "                    else:\n",
    "                        ratio = wldims[0] / wldims[2]\n",
    "                        batch_size = batch_size / ratio\n",
    "                        wldims[2] = wldims[0] = wldims[0] * ((batch_size) ** (1/2))\n",
    "\n",
    "                # Step 1: find the tiling dims when all 3 dims are flexible\n",
    "                t_dim = estimate_tiling_sizes(L3_cache_capacity, bytes_per_value, 0)\n",
    "                dims = [(0, wldims[0]), (1, wldims[1]), (2, wldims[2])]\n",
    "                dims_sorted = sorted(dims, key=lambda x: x[1])\n",
    "                if dims_sorted[0][1] >= t_dim:\n",
    "                    tile_dims = [(0, t_dim), (1, t_dim), (2, t_dim)]\n",
    "                else: # Step 2: find the tiling dims when 1 dim (the smallest) is fixed\n",
    "                    t_dim = estimate_tiling_sizes(L3_cache_capacity, bytes_per_value, 1, dims_sorted[0][1])\n",
    "                    if dims_sorted[1][1] >= t_dim:\n",
    "                        dims_sorted[1] = (dims_sorted[1][0], t_dim)\n",
    "                        dims_sorted[2] = (dims_sorted[2][0], t_dim)\n",
    "                        tile_dims = sorted(dims_sorted, key=lambda x: x[0])\n",
    "                    else: # Step 3: find the last tiling dim when the smallest 2 dims are fixed\n",
    "                        t_dim = estimate_tiling_sizes(L3_cache_capacity, bytes_per_value, 2, dims_sorted[0][1], dims_sorted[1][1])\n",
    "                        if dims_sorted[2][1] >= t_dim:\n",
    "                            dims_sorted[2] = (dims_sorted[2][0], t_dim)\n",
    "                            tile_dims = sorted(dims_sorted, key=lambda x: x[0])\n",
    "                        else:\n",
    "                            tile_dims = sorted(dims_sorted, key=lambda x: x[0])\n",
    "\n",
    "                best_tiler = f\"{tile_dims[0][1]:.1f}x{tile_dims[1][1]:.1f}x{tile_dims[2][1]:.1f}\"\n",
    "\n",
    "                # Step 4: compare the cmp_cycle and the mem_cycle and decide the bound\n",
    "                block_compute = tile_dims[0][1] * tile_dims[1][1] * tile_dims[2][1]\n",
    "                cmp_cycle = block_compute / HW_SPEC[\"MACs_per_second\"] * HW_SPEC[\"Frequency\"] * layer[\"compute\"]/block_compute\n",
    "                memory = (tile_dims[0][1] * tile_dims[1][1] + tile_dims[2][1] * tile_dims[1][1]) * bytes_per_value\n",
    "                mem_cycle = memory / HW_SPEC[\"Bandwidth\"] * HW_SPEC[\"Frequency\"] * layer[\"compute\"]/block_compute\n",
    "                mem_cycle_to_cmp_cycle = mem_cycle / cmp_cycle if cmp_cycle else np.nan\n",
    "\n",
    "                if cmp_cycle > mem_cycle:\n",
    "                    exec_cycle = cmp_cycle\n",
    "                    layer_bound = \"Compute\"\n",
    "                else:\n",
    "                    exec_cycle = mem_cycle\n",
    "                    layer_bound = \"Memory\"\n",
    "            else:\n",
    "                exec_cycle = np.nan\n",
    "                best_tiler = None\n",
    "                cmp_cycle = np.nan\n",
    "                mem_cycle = np.nan\n",
    "                mem_cycle_to_cmp_cycle = np.nan\n",
    "        elif layer_type == KVCache:\n",
    "            if not pd.isna(layer[\"kv_cache\"]):\n",
    "                memory = layer[\"kv_cache\"] * bytes_per_value\n",
    "                exec_cycle = memory / HW_SPEC[\"Bandwidth\"] * HW_SPEC[\"Frequency\"]\n",
    "                layer_bound = \"Memory\"\n",
    "            else:\n",
    "                exec_cycle = np.nan\n",
    "        elif layer_type == Async_KVCache:\n",
    "            exec_cycle = np.nan\n",
    "        else:\n",
    "            exec_cycle = np.nan\n",
    "        # Update DataFrame in place\n",
    "        wl.layers.at[idx, \"best_tiler\"] = best_tiler\n",
    "        wl.layers.at[idx, \"exec_cycle\"] = exec_cycle\n",
    "        wl.layers.at[idx, \"cmp_cycle\"] = cmp_cycle\n",
    "        wl.layers.at[idx, \"mem_cycle\"] = mem_cycle\n",
    "        wl.layers.at[idx, \"mem_cycle_to_cmp_cycle\"] = mem_cycle_to_cmp_cycle\n",
    "        wl.layers.at[idx, \"bound\"] = layer_bound\n",
    "    return wl.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15452045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 8192\n",
      "Prefill Compute 2336462209024.0\n",
      "Prefill Weights 218103808.0\n",
      "Prefill KV cache 16777216.0\n",
      "            type           name     m      k      n batch_size grouped_q  \\\n",
      "0          Ma_Mw        Q/K/V*W  8192   4096   6144          1         1   \n",
      "1         Ma_Mqa           Q*gK  8192   4096   8192          1         1   \n",
      "2         Ma_Mqa        Q*gK*gV  8192   4096   8192          1         1   \n",
      "3          Ma_Mw            O*W  8192   4096   4096          1         1   \n",
      "4          Ma_Mw         FFN_up  8192   4096  14336          1         1   \n",
      "5          Ma_Mw       FFN_gate  8192   4096  14336          1         1   \n",
      "6          Ma_Mw       FFN_down  8192  14336   4096          1         1   \n",
      "7  Async_KVCache  KVCache_Store  8192    128     16          1         1   \n",
      "\n",
      "        compute      weight    kv_cache  \n",
      "0  2.061584e+11  25165824.0         NaN  \n",
      "1  2.748779e+11         NaN         NaN  \n",
      "2  2.748779e+11         NaN         NaN  \n",
      "3  1.374390e+11  16777216.0         NaN  \n",
      "4  4.810363e+11  58720256.0         NaN  \n",
      "5  4.810363e+11  58720256.0         NaN  \n",
      "6  4.810363e+11  58720256.0         NaN  \n",
      "7           NaN         NaN  16777216.0  \n",
      "\n",
      "   Details:\n",
      "   === Input parameters ===\n",
      "   Context length: 8192\n",
      "   Bytes per FP value: 2\n",
      "   Layers: 32\n",
      "   Total Compute (MACs): 68.0T\n",
      "   Total Weights Footprint (Bytes): 6.5G\n",
      "   Total KV Cache Footprint (Bytes): 512.0M\n",
      "            type           name     m      k      n batch_size grouped_q  \\\n",
      "0          Ma_Mw        Q/K/V*W  8192   4096   6144          1         1   \n",
      "1         Ma_Mqa           Q*gK  8192   4096   8192          1         1   \n",
      "2         Ma_Mqa        Q*gK*gV  8192   4096   8192          1         1   \n",
      "3          Ma_Mw            O*W  8192   4096   4096          1         1   \n",
      "4          Ma_Mw         FFN_up  8192   4096  14336          1         1   \n",
      "5          Ma_Mw       FFN_gate  8192   4096  14336          1         1   \n",
      "6          Ma_Mw       FFN_down  8192  14336   4096          1         1   \n",
      "7  Async_KVCache  KVCache_Store  8192    128     16          1         1   \n",
      "\n",
      "        compute      weight    kv_cache            best_tiler  exec_cycle  \\\n",
      "0  2.061584e+11  25165824.0         NaN  2508.3x2508.3x2508.3   3145728.0   \n",
      "1  2.748779e+11         NaN         NaN  2508.3x2508.3x2508.3   4194304.0   \n",
      "2  2.748779e+11         NaN         NaN  2508.3x2508.3x2508.3   4194304.0   \n",
      "3  1.374390e+11  16777216.0         NaN  2508.3x2508.3x2508.3   2097152.0   \n",
      "4  4.810363e+11  58720256.0         NaN  2508.3x2508.3x2508.3   7340032.0   \n",
      "5  4.810363e+11  58720256.0         NaN  2508.3x2508.3x2508.3   7340032.0   \n",
      "6  4.810363e+11  58720256.0         NaN  2508.3x2508.3x2508.3   7340032.0   \n",
      "7           NaN         NaN  16777216.0                  None         NaN   \n",
      "\n",
      "   cmp_cycle     mem_cycle  mem_cycle_to_cmp_cycle    bound  \n",
      "0  3145728.0  2.238203e+06                0.711506  Compute  \n",
      "1  4194304.0  2.984271e+06                0.711506  Compute  \n",
      "2  4194304.0  2.984271e+06                0.711506  Compute  \n",
      "3  2097152.0  1.492136e+06                0.711506  Compute  \n",
      "4  7340032.0  5.222474e+06                0.711506  Compute  \n",
      "5  7340032.0  5.222474e+06                0.711506  Compute  \n",
      "6  7340032.0  5.222474e+06                0.711506  Compute  \n",
      "7        NaN           NaN                     NaN     None  \n",
      "Decode Compute 73551314944.0\n",
      "Decode Weights 218103808.0\n",
      "Decode KV cache 4429185024.0\n",
      "      type          name     m      k      n batch_size grouped_q  \\\n",
      "0    Va_Mw       Q/K/V*W     1   4096   6144        256         1   \n",
      "1  KVCache  KVCache_Read  8448    128     16        256         1   \n",
      "2   Va_Mqa          Q*gK     1   4096   8448        256         1   \n",
      "3   Va_Mqa       Q*gK*gV     1   4096   8448        256         1   \n",
      "4    Va_Mw           O*W     1   4096   4096        256         1   \n",
      "5    Va_Mw        FFN_up     1   4096  14336        256         1   \n",
      "6    Va_Mw      FFN_gate     1   4096  14336        256         1   \n",
      "7    Va_Mw      FFN_down     1  14336   4096        256         1   \n",
      "\n",
      "        compute      weight      kv_cache  \n",
      "0  6.442451e+09  25165824.0           NaN  \n",
      "1           NaN         NaN  4.429185e+09  \n",
      "2  8.858370e+09         NaN           NaN  \n",
      "3  8.858370e+09         NaN           NaN  \n",
      "4  4.294967e+09  16777216.0           NaN  \n",
      "5  1.503239e+10  58720256.0           NaN  \n",
      "6  1.503239e+10  58720256.0           NaN  \n",
      "7  1.503239e+10  58720256.0           NaN  \n",
      "\n",
      "   Details:\n",
      "   === Input parameters ===\n",
      "   Context length: 8448\n",
      "   Bytes per FP value: 2\n",
      "   Layers: 32\n",
      "   Total Compute (MACs): 2.1T\n",
      "   Total Weights Footprint (Bytes): 6.5G\n",
      "   Total KV Cache Footprint (Bytes): 132.0G\n",
      "      type          name     m      k      n batch_size grouped_q  \\\n",
      "0    Va_Mw       Q/K/V*W     1   4096   6144        256         1   \n",
      "1  KVCache  KVCache_Read  8448    128     16        256         1   \n",
      "2   Va_Mqa          Q*gK     1   4096   8448        256         1   \n",
      "3   Va_Mqa       Q*gK*gV     1   4096   8448        256         1   \n",
      "4    Va_Mw           O*W     1   4096   4096        256         1   \n",
      "5    Va_Mw        FFN_up     1   4096  14336        256         1   \n",
      "6    Va_Mw      FFN_gate     1   4096  14336        256         1   \n",
      "7    Va_Mw      FFN_down     1  14336   4096        256         1   \n",
      "\n",
      "        compute      weight      kv_cache           best_tiler    exec_cycle  \\\n",
      "0  6.442451e+09  25165824.0           NaN  256.0x4096.0x4096.0  3.640694e+05   \n",
      "1           NaN         NaN  4.429185e+09                 None  6.030702e+07   \n",
      "2  8.858370e+09         NaN           NaN  256.0x4096.0x4096.0  5.005954e+05   \n",
      "3  8.858370e+09         NaN           NaN  256.0x4096.0x4096.0  5.005954e+05   \n",
      "4  4.294967e+09  16777216.0           NaN  256.0x4096.0x4096.0  2.427129e+05   \n",
      "5  1.503239e+10  58720256.0           NaN  256.0x4096.0x4096.0  8.494952e+05   \n",
      "6  1.503239e+10  58720256.0           NaN  256.0x4096.0x4096.0  8.494952e+05   \n",
      "7  1.503239e+10  58720256.0           NaN  256.0x4096.0x4096.0  8.494952e+05   \n",
      "\n",
      "   cmp_cycle      mem_cycle  mem_cycle_to_cmp_cycle   bound  \n",
      "0    98304.0  364069.353070                3.703505  Memory  \n",
      "1        NaN            NaN                     NaN  Memory  \n",
      "2   135168.0  500595.360471                3.703505  Memory  \n",
      "3   135168.0  500595.360471                3.703505  Memory  \n",
      "4    65536.0  242712.902047                3.703505  Memory  \n",
      "5   229376.0  849495.157164                3.703505  Memory  \n",
      "6   229376.0  849495.157164                3.703505  Memory  \n",
      "7   229376.0  849495.157164                3.703505  Memory  \n",
      "Chunked Compute 65071669248.0\n",
      "Chunked Weights 218103808.0\n",
      "Chunked KV cache 269467648.0\n",
      "             type           name     m      k      n batch_size grouped_q  \\\n",
      "0           Ma_Mw        Q/K/V*W   256   4096   6144          1         1   \n",
      "1         KVCache  dKVCache_Read  8448    128     16         15         1   \n",
      "2          Va_Mqa          dQ*gK     1   4096   1024         15         1   \n",
      "3          Va_Mqa       dQ*gK*gV     1   4096   1024         15         1   \n",
      "4   Async_KVCache  pKVCache_Read  4615    128     16          1         1   \n",
      "5          Ma_Mqa          pQ*gK   241   4096   4615          1         1   \n",
      "6          Ma_Mqa       pQ*gK*gV   241   4096   4615          1         1   \n",
      "7   Async_KVCache  KVCache_Store   241    128     16          1         1   \n",
      "8           Ma_Mw            O*W   256   4096   4096          1         1   \n",
      "9           Ma_Mw         FFN_up   256   4096  14336          1         1   \n",
      "10          Ma_Mw       FFN_gate   256   4096  14336          1         1   \n",
      "11          Ma_Mw       FFN_down   256  14336   4096          1         1   \n",
      "\n",
      "         compute      weight     kv_cache  \n",
      "0   6.442451e+09  25165824.0          NaN  \n",
      "1            NaN         NaN  259522560.0  \n",
      "2   6.291456e+07         NaN          NaN  \n",
      "3   6.291456e+07         NaN          NaN  \n",
      "4            NaN         NaN    9451520.0  \n",
      "5   4.555633e+09         NaN          NaN  \n",
      "6   4.555633e+09         NaN          NaN  \n",
      "7            NaN         NaN     493568.0  \n",
      "8   4.294967e+09  16777216.0          NaN  \n",
      "9   1.503239e+10  58720256.0          NaN  \n",
      "10  1.503239e+10  58720256.0          NaN  \n",
      "11  1.503239e+10  58720256.0          NaN  \n",
      "\n",
      "   Details:\n",
      "   === Input parameters ===\n",
      "   Prefill context length: 4615\n",
      "   Decode context length: 8448\n",
      "   Bytes per FP value: 2\n",
      "   Layers: 32\n",
      "   Total Compute (MACs): 1.9T\n",
      "   Total Weights Footprint (Bytes): 6.5G\n",
      "   Total KV Cache Footprint (Bytes): 8.0G\n",
      "             type           name     m      k      n batch_size grouped_q  \\\n",
      "0           Ma_Mw        Q/K/V*W   256   4096   6144          1         1   \n",
      "1         KVCache  dKVCache_Read  8448    128     16         15         1   \n",
      "2          Va_Mqa          dQ*gK     1   4096   1024         15         1   \n",
      "3          Va_Mqa       dQ*gK*gV     1   4096   1024         15         1   \n",
      "4   Async_KVCache  pKVCache_Read  4615    128     16          1         1   \n",
      "5          Ma_Mqa          pQ*gK   241   4096   4615          1         1   \n",
      "6          Ma_Mqa       pQ*gK*gV   241   4096   4615          1         1   \n",
      "7   Async_KVCache  KVCache_Store   241    128     16          1         1   \n",
      "8           Ma_Mw            O*W   256   4096   4096          1         1   \n",
      "9           Ma_Mw         FFN_up   256   4096  14336          1         1   \n",
      "10          Ma_Mw       FFN_gate   256   4096  14336          1         1   \n",
      "11          Ma_Mw       FFN_down   256  14336   4096          1         1   \n",
      "\n",
      "         compute      weight     kv_cache           best_tiler    exec_cycle  \\\n",
      "0   6.442451e+09  25165824.0          NaN  256.0x4096.0x4096.0  3.640694e+05   \n",
      "1            NaN         NaN  259522560.0                 None  3.533614e+06   \n",
      "2   6.291456e+07         NaN          NaN   15.0x4096.0x1024.0  5.794547e+04   \n",
      "3   6.291456e+07         NaN          NaN   15.0x4096.0x1024.0  5.794547e+04   \n",
      "4            NaN         NaN    9451520.0                 None           NaN   \n",
      "5   4.555633e+09         NaN          NaN  241.0x4096.0x4124.3  2.724202e+05   \n",
      "6   4.555633e+09         NaN          NaN  241.0x4096.0x4124.3  2.724202e+05   \n",
      "7            NaN         NaN     493568.0                 None           NaN   \n",
      "8   4.294967e+09  16777216.0          NaN  256.0x4096.0x4096.0  2.427129e+05   \n",
      "9   1.503239e+10  58720256.0          NaN  256.0x4096.0x4096.0  8.494952e+05   \n",
      "10  1.503239e+10  58720256.0          NaN  256.0x4096.0x4096.0  8.494952e+05   \n",
      "11  1.503239e+10  58720256.0          NaN  256.0x4096.0x4096.0  8.494952e+05   \n",
      "\n",
      "      cmp_cycle      mem_cycle  mem_cycle_to_cmp_cycle   bound  \n",
      "0    98304.0000  364069.353070                3.703505  Memory  \n",
      "1           NaN            NaN                     NaN  Memory  \n",
      "2      960.0000   57945.474547               60.359869  Memory  \n",
      "3      960.0000   57945.474547               60.359869  Memory  \n",
      "4           NaN            NaN                     NaN    None  \n",
      "5    69513.4375  272420.218796                3.918958  Memory  \n",
      "6    69513.4375  272420.218796                3.918958  Memory  \n",
      "7           NaN            NaN                     NaN    None  \n",
      "8    65536.0000  242712.902047                3.703505  Memory  \n",
      "9   229376.0000  849495.157164                3.703505  Memory  \n",
      "10  229376.0000  849495.157164                3.703505  Memory  \n",
      "11  229376.0000  849495.157164                3.703505  Memory  \n",
      "Total compute-bound cycles: 0\n",
      "Total memory-bound cycles: 7349613\n"
     ]
    }
   ],
   "source": [
    "### Output \n",
    "\n",
    "# Estimated execution time for different kernels (P2)\n",
    "# Estimated activities for different kernels for power analysis (P6)\n",
    "\n",
    "# Breakdown between different kernels and the shape of the tensors: Mw_Ma, Ma_Ma, Mw_Va, Vw_Va, Va_Va\n",
    "input_length = 8192\n",
    "output_length = 512\n",
    "p_batch_size = 1\n",
    "d_batch_size = 256\n",
    "chunksize = 256\n",
    "d_tokens = chunksize * output_length // (input_length + output_length)\n",
    "p_tokens = chunksize - d_tokens\n",
    "d_context_length = input_length + output_length // 2    # This is used to approximately the average decode computation\n",
    "p_context_length = int(input_length / 1.73 - p_tokens//2)             # 1.73 = sqrt(3) This is used to approximately the average prefill computation\n",
    "\n",
    "print(\"Input length:\", input_length)\n",
    "wl = perform_prefill(llm, input_length, p_batch_size)\n",
    "kernel_times = estimate_kernel_times_by_theoretical_tiler(wl, HW_SPEC)\n",
    "print (wl.layers)\n",
    "\n",
    "wl = perform_decode(llm, d_context_length, d_batch_size)    \n",
    "kernel_times = estimate_kernel_times_by_theoretical_tiler(wl, HW_SPEC)\n",
    "print (wl.layers)\n",
    "\n",
    "wl = perform_chunkedprefill(llm, p_context_length, p_tokens, d_context_length, d_tokens)    \n",
    "kernel_times = estimate_kernel_times_by_theoretical_tiler(wl, HW_SPEC)\n",
    "print (wl.layers)\n",
    "\n",
    "compute_cycles = kernel_times.loc[kernel_times[\"bound\"] == \"Compute\", \"exec_cycle\"].sum()\n",
    "memory_cycles = kernel_times.loc[kernel_times[\"bound\"] == \"Memory\", \"exec_cycle\"].sum()\n",
    "\n",
    "print(f\"Total compute-bound cycles: {compute_cycles:.0f}\")\n",
    "print(f\"Total memory-bound cycles: {memory_cycles:.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
